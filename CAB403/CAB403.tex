\documentclass[oneside]{book}
\usepackage{glossaries}
\usepackage{derivative}
\usepackage{geometry}
\usepackage{parskip}
\usepackage{float}
\usepackage{subcaption}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{graphicx}
\usepackage{lipsum}
\usepackage{minted}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[hidelinks]{hyperref}
\geometry{
    left=10mm,
    right=10mm,
    top=12mm,
    bottom=12mm,
}
\begin{document}
\pagestyle{fancy}
    \fancyhf{}
\fancyhead[L]{CAB403 - Systems Programming}
\fancyhead[R]{\nouppercase{\leftmark}}
\fancyfoot[L]{Dinal Atapattu}
\renewcommand{\footrulewidth}{0.4pt}
\fancyfoot[R]{Page \thepage\ of \pageref{LastPage}}
    \title{
            Queensland University of Technology\\
            \rule{\linewidth}{0.5pt}
        \centering
        \textbf{CAB403} \\
        Systems Programming\\
        \vspace{0.4cm}
        \rule{\linewidth}{1.5pt}
        \small{\textit{Professor Timothy Chappell}}
    }
    \author{Dinal Atapattu}
    \date{\today}
    \maketitle
    \tableofcontents
    \chapter{Introduction to Operating Systems}
        \section{Operating System Structures}
            \subsection{Operating System Services}
                \begin{itemize}
                    \item Operating systems provide an environment for execution of programs and services to programs
                    and users
                    \item Operating System services provides functions that are helpful to the user
                        \begin{itemize}
                            \item User Interface - Almost all opeating systems have a user interface (UI)
                            \begin{itemize}
                                \item Graphical (GUI)
                                \item Command Line (CLI)
                                \item Batch
                            \end{itemize}
                            \item Program Execution - The system must be able to load a program into memory and run that
                            program, end execution, either normally or abnormally (indicating error)
                            \item I/O operations - A running program may require I/O, which involves either a file or I/O device
                        \end{itemize}
                \end{itemize}
    \chapter{Operating System Structures}
    \chapter{Processes}
    \chapter{Threads}
            A thread is a fundamental unit of CPU utilization, which forms the basis of multithreaded computer systems.
            Many modern applications are multithreaded, with threads running within an application. An application can divide tasks to seperate threads
            such as
            \begin{itemize}
                \item Updating the display
                \item Fetching data
                \item Spell checking
                \item Answering network requests
            \end{itemize}
            Thread creation, as opposed to process creation is lightweight, can simplify code, and increase efficiency. For this reason, kernels are often
            multithreaded.
            \begin{figure}[H]
                \centering
                \includegraphics{figures/multithreader_server.pdf}
                \caption{Multithreaded Server Architecture}
            \end{figure}
            Multithreading has the following benefits
            \begin{itemize}
                \item \textbf{Responsiveness}
                    \subitem May allow continued execution if part of a process is blocked, important with user interfaces.
                \item \textbf{Resource Sharing}
                    \subitem Threads share resources of process, easier to manage than shared memory or message parsing.
                \item \textbf{Economy}
                    \subitem Cheaper than process creation, thread switching has lower overhead than context switching.
                \item \textbf{Scalability}
                    \subitem Processes can take advantage of a multiprocessor architecture
            \end{itemize}
        \section{Multicore Programming}
            In the early history of computer design, in order to combat the need for increased computing performance, single-CPU systems
            evoled into mult-CPU systems. Later, this evolved into including multiple compute cores on a single processing chip, where each core
            appears as a seperate CPU to the operating systems. These systems are defined as \textbf{multicore}.
            \subsection{Programming Challenges}
                The trend towards multicore systems continually places pressure on system designers and programmers to make better use of
                multiple compute cores. Designers of operating systems must write scheduling algorithms that use multiple processing cores
                to allow parallel execution
                \begin{figure}[H]
                    \centering
                    \includegraphics{figures/multicore_exec.pdf}
                    \caption{Parallel Execution on a Multicore System}
                \end{figure}
            In general, five areas present challenges in programming for multicore systems
            \begin{itemize}
                \item Identifying tasks
                    \subitem This involves examining applications to find areas that can be divided into seperate, concurrent tasks. Ideally, tasks
                    are independent of one another and thus can run in parallel on individual cores
                \item Balance
                    \subitem While Identifying tasks that run in parallel, programmers msut also ensure that tasks perform equal work of equal value.
                    In some instances, a certain task may not contribute as much value to the overall process as other tasks. Using seperate execution cores
                    for that task might not be worth the cost
                \item Data Splitting
                    \subitem Data accessed and manipulated by tasks must be divded to run on seperate cores, similar to how applications are divided to seperate tasks
                \item Data Dependency
                    \subitem The data accessed by he tasks must be examined for dependencies between the two or more tasks. When data is dependent between cores,
                    programmers must ensure that the execution of the tasks is synchronized to accomodate the dependency.
                    \begin{figure}[H]
                        \centering
                        \includegraphics{figures/data_task_parallelism.pdf}
                        \caption{Data and Task Parallism}
                    \end{figure}
                \item Testing and Debugging
                    \subitem When a program is running in parallel n multiple cores, many different execution paths are possible. Testing and debugging such
                    concurrent programs is inherently more difficult than testing and debugging single threaded applications
            \end{itemize}
            \subsection{Parallelism}
                Types of parallelism
                \begin{itemize}
                    \item Data Parallelism
                        \subitem Focuses on distributing subsets of the same data across multiple compute cores, performing the same operation on each core.
                        \subitem Example: summing the contents of an array size $N$, on a dual core system, thread A sums the elements [0] ... [$N/2 - 1$], thread B
                        sums the elements $[N/2]$ ... $[N-1]$. These threads will run in parallel on seperate cores.
                    \item Task Parallelism
                        \subitem Distributes tasks across multiple cores. Each thread peforms a unique operation. Different threads may operate on the same or different
                        data.
                        \subitem Example: Dual core system, applying two different arithmetic and/or other operation on the same block of data on seperate threads. These
                        threads will will run parallel on seperate cores.
                \end{itemize}
                Data and Task parallelism may be done together, in a hybrid solution.
                \begin{figure}[H]
                    \centering
                    \includegraphics{figures/data_spaces.pdf}
                    \caption{Use and Kernel Data Spaces}
                \end{figure}
        \section{Multithreading Models}
            \subsection{Many-To-One Model}
                Many user-level threads are mapped to a single kernel thread.\\
                One thread block causes all to block.\\
                Multiple threads cannot run in parallel on a multicore system because the kernel can only handle a single thread.\\
                Rarely used.\\
                Examples include
                \begin{itemize}
                    \item Solaris Green threads
                    \item GNU Portable threads
                \end{itemize}
                \begin{figure}[H]
                    \centering
                    \includegraphics{figures/many_to_one.pdf}
                    \caption{Many-to-one model}
                \end{figure}
            \subsection{One-To-One Model}
                Each user-level thread maps to a kernel thread.\\
                Creating a user level thread creates a kernel thread.\\
                More concurrency than many-to-one.\\
                Number of threads per process sometimes restricted due to overhead.\\
                Examples include.
                \begin{itemize}
                    \item Windows
                    \item Linux
                    \item Solaris (9 and later)
                \end{itemize}
                \begin{figure}[H]
                    \centering
                    \includegraphics{figures/one_to_one.pdf}
                    \caption{One-to-one model} 
                \end{figure}
            \subsection{Many-to-many model}
                Allows many user level threads to be mapped to many kernel threads.\\
                Allows the operating system to create a sufficient number of kernel threads.\\
                Examples include
                \begin{itemize}
                    \item Solaris (pre version 9)
                    \item Windows (\textit{ThreadFiber} package)
                \end{itemize}
                \begin{figure}[H]
                    \centering
                    \includegraphics{figures/many_to_many.pdf}
                    \caption{Many-to-many model}
                \end{figure}
            \subsection{Two-level model}
                Similar to Many-to-many, except allows a user thread to be \textbf{bound} to a kernel thread
                Examples include
                \begin{itemize}
                    \item IRIX
                    \item HP-UX
                    \item Tru64 UNIX
                    \item Solaris (8 and earlier)
                \end{itemize}
                \begin{figure}[H]
                    \centering
                    \includegraphics{figures/two_level.pdf}
                    \caption{Two-level model}
                \end{figure}
        \section{Thread Libraries}
            \subsection{Pthreads}
                Refers to the POSIX standard (IEEE 1003.1c) defining an API for thread creation and synchronisation.
                This is a specification not an implementation
                \begin{figure}[H]
                    \centering
                    \inputminted{c}{code/threads/pthreads.c}
                    \caption{Multithreading using the pthreads API}
                \end{figure}
                Although Windows does not natively support pthreads, some third-party implementations are available
                \begin{figure}[H]
                    \centering
                    \begin{minted}{c}
                        #define NUM_THREADS 10
                        /* an array of threads to be joined upon */
                        pthread_t workers[NUM_THREADS];

                        for (int i = 0; i < NUM_THREADS; i++)
                        {
                            pthread_join(workers[i], NULL);
                        }
                    \end{minted}
                    \caption{Joining 10 threads using the pthreads API}
                \end{figure}
            \subsection{Windows Threads}
                Similar to the technique used in pthreads. Uses a different library
                \begin{figure}[H]
                    \centering
                    \inputminted{c}{code/threads/windows_threads.c}
                    \caption{Multithreading using the Windows API}
                \end{figure}
        \section{Implicit Threading}
            The growing popularity of multicore processing means that applications now require hundreds,
            or thousands of threads. When designing such programs, correctness grows more and more difficult.
            Creating and management of threads done by compilers and run-time libraries are favoured in this case.
            \subsection{Thread Pools}
                Creates a number of threads in a pool where they await work
                Advantages
                \begin{itemize}
                    \item Usually faster to service a request with an existing thread than create a new thread
                    \item Alows the number of threads in an application(s) to be bound to the size of the pool
                    \item Seperating tasks to be performed from mechanics of creating task allows different stratgeies for
                    running task
                        \subitem Tasks could be scheduled to be run periodically
                \end{itemize}
                \begin{figure}[H]
                    \centering
                    \begin{minted}{c}
                        DWORD WINAPI PoolFunction(PVOID Param)
                        {
                            /*
                            * this function runs as a seperate thread
                            */
                        }
                    \end{minted}
                    \caption{Thread pooling in the Windows API}
                \end{figure}
            \subsection*{OpenMP}
                Set of compiler directives and an API for C, C++, FORTRAN.\\
                Provides support for parallel programming and shared-memory environments.\\
                Identifies \textcolor{blue}{parallel regions} - blocks of code that can run in parallel.\\
                \begin{figure}[H]
                    \centering
                    \inputminted{c}{code/threads/openmp.c}
                    \caption{OpenMP}
                \end{figure}
            \subsection{Grand Central Dispatch}
                \begin{itemize}
                    \item Apple technology for Mac OS X and iOS operating systems
                    \item Extensions to C, C++, API and run-time library
                    \item Allows identification of parallel sections
                    \item Manages most details of threading
                    \item Blocks is in "\textasciicircum\{\}" - \mintinline{c}{^{ printf("I am a block"); }}
                    \item Blocks are placed in dispatch queue and then assigned to avaiable threads in thread pool when removed
                    \item Two types of dispatch queues
                        \begin{itemize}
                            \item \textbf{Serial} - blocks are removed FIFO, queue is per process, called \textbf{Main Queue}
                                \subitem Programmers create additional serial queues within program
                            \item \textbf{Concurrent} - removed in FIFO, but many be removed at a time
                                \subitem Three system wide queues with priorities \textcolor{blue}{low, default, high}
                                \begin{figure}[H]
                                    \begin{flushleft}
                                        \begin{minted}{c}
dispatch_queue_t queue = dispatch_get_global_queue(DISPATCH_QUEUE_PRIORITY_DEFAULT, 0);
dispatch_async(queue, ^{ printf("I am a block."); });
                                        \end{minted}
                                    \end{flushleft}
                                \end{figure}
                        \end{itemize}
                \end{itemize}
        \section{Threading Issues}
            \begin{itemize}
                \item Semantics of \textbf{fork()} and \textbf{exec()} system calls.
                \item Signal handling (synchronous and asynchronous)
                \item Thread cancellation of target thread (asynchronous or deferred)
                \item Thread local storage
                \item Scheduler activations
            \end{itemize}
            \subsection{Semantics of fork() and exec()}
                \begin{itemize}
                    \item Does fork() duplicate only the calling thread or all threads?
                        \subitem Some UNIXes have two versions of fork
                    \item exec() usually works as normal (replaces running process including all threads)
                \end{itemize}
            \subsection{Signal Handling}
                \begin{itemize}
                    \item \textcolor{blue}{Signals} are used in UNIX systems to notify a process that a particular event has occurred
                    \item A \textcolor{blue}{signal handler} is used to process signals
                        \begin{itemize}
                            \item Signal is generated by an event
                            \item Signal is delivered to a process
                            \item Signal is handled by one of two signal handlers
                                \subitem default
                                \subitem user-defined
                        \end{itemize}
                    \item Every signal has a \textcolor{blue}{default handler} that kernel runs when handling a signal
                        \begin{itemize}
                            \item \textcolor{blue}{User-defined signal handler} can override default
                            \item For single-threaded, signal delivered to process
                        \end{itemize}
                    \item Where is the signal delivered in multi-threaded?
                        \begin{itemize}
                            \item Deliver the signal to the thread to which the signal applies
                            \item Deliver the signal to every thread in the process
                            \item Deliver the signal to certain threads in process
                        \end{itemize}
                \end{itemize}
            \subsection{Thread Cancellation}
                \begin{itemize}
                    \item Terminating a thread before it has finished
                    \item Thread to be canceled is \textcolor{blue}{target thread}
                    \item Two general approaches
                        \begin{itemize}
                            \item \textbf{Asynchronous cancellation} terminates the target thread immediately
                            \item \textbf{Deferred cancellation} allows the target thread to periodically check if it should be cancelled
                        \end{itemize}
                        \begin{figure}[H]
                            \begin{minted}{c}
                                pthread_t thread_id;
                                /* create the thread */
                                pthread_create(&thread_id, 0, worker, NULL);
                                ...
                                /* cancel the thread */
                                pthread_cancel(thread_id);
                            \end{minted}
                            \caption{pthread code to create and cancel a thread}
                        \end{figure}
                    \item Invoking thread cancellation requests cancellation, but actual cancellation depends on the thread state
                        \begin{figure}[H]
                            \centering
                            \begin{tabular}{|c|c|c|}
                                \hline
                                \textbf{Mode} & \textbf{State} & \textbf{Type}\\
                                \hline
                                Off & Disabled & -\\
                                \hline
                                Deferred & Enabled & Deferred\\
                                \hline
                                Asynchronous & Enabled & Asynchronous\\
                                \hline
                            \end{tabular}
                            \caption{Thread States}
                        \end{figure}
                    \item If a thread has cancellation disabled, cancellation remains pending till enabled
                    \item Default type is deferred
                        \subitem Cancellation only occurs when thread reaches \textcolor{blue}{cancellation point}
                            \begin{itemize}
                                \item i.e., \mintinline{c}{pthread_testcancel();}
                                \item Then \textcolor{blue}{cleanup handler} is invoked
                            \end{itemize}
                    \item On Linux, thread cancellation is handled through signals
                \end{itemize}
            \subsection{Thread-Local Storage}
                \begin{itemize}
                    \item \textcolor{blue}{Thread-local-storage (TLS)} allows each thread to have its own copy of data
                    \item Useful when you don't have control over thread creation (i.e., using a thread pool)
                    \item Different from local variables
                        \begin{itemize}
                            \item Local variables visible only during single function invocation
                            \item TLS visible across function invocations
                        \end{itemize}
                    \item Similar to static data
                        \begin{itemize}
                            \item TLS is unique to each thread
                        \end{itemize}
                \end{itemize}
            \subsection{Scheduler Activations}
                \begin{itemize}
                    \item Both Many to Many and Two-Level require communication to maintain the appropriate 
                    number of kernel threads allocated to the application
                    \item Typically use an intermediate data structure between user and kernel threads (\textcolor{blue}{lightweight process (LWP)})
                        \begin{itemize}
                            \item Appears to be a virtual processor on which process can schedule user thread to run
                            \item Each LWP is attached to a kernel thread
                            \item How many LWPs to create?
                        \end{itemize}
                    \item Scheduler activations provide \textcolor{blue}{upcalls} - a communication mechanism from the kernel
                    to the \textcolor{blue}{upcall handler} in the thread library
                    \item This communication allows an application to maintain the correct number of kernel threads
                \end{itemize}
                \begin{figure}[H]
                    \centering
                    \includegraphics{figures/lwp.pdf}
                    \caption{Lightweight Process (LWP)}
                \end{figure}
    \chapter{Synchronisation}
        Processes can either execute concurrently or in parallel. These processes may be 
        interrupted at any time, partially completing execution.
        Concurrent access to any shared memory may result in data inconsistency if not properly
        controlled.\\
        Maintaining data consistency requires mechanisms to ensure the orderly execution of 
        cooperating processes.

        \hangindent=1cm Example: \\Suppose we wanted to provide a solution to the consumer-product problem that
        fills \textbf{all} the buffers. We can do so by having an integer \texttt{counter} that keeps
        track of the number of full buffers. Initially, \texttt{counter} is set to 0. It is incremented
        by the producer after producing a new buffer, and decremented by the consumer after it consumes
        a buffer.
        % side by side for producer consumer 2 figures each code blocks of C
        \begin{figure}[H]
            \centering
            \begin{minipage}{0.49\textwidth}
                \begin{minted}{c}
                    while (true) {
                        /* produce an item in next produced */
                        while (counter == BUFFER_SIZE);
                            /* do nothing */
                        buffer[in] = next_produced;
                        in = (in + 1) % BUFFER_SIZE;
                        counter++;
                    }
                \end{minted}
            \end{minipage}
            \begin{minipage}{0.49\textwidth}
                \begin{minted}{c}
                    while (true) {
                        while (counter == 0);
                            /* do nothing */
                        next_consumed = buffer[out];
                        out = (out + 1) % BUFFER_SIZE;
                        counter--;
                        /* consume the item in next_consumed */
                    }
                \end{minted}
            \end{minipage}
            \caption{Producer and Consumer}
        \end{figure}

        \section{Race Conditions}
        While the producer and consumer routines shown are correct seperately,
        they may not function correctly when executed concurrently.
        For example: Suppose that the value of the variable \texttt{count} is currently
        5 and the producer and consumer processes concurrently execute the statements
        "\texttt{count++}" and "\texttt{count--}". This will lead to the variable 
        being 4, 5 or 6. With the only correct value being \texttt{count == 5}, which 
        is executed if the producer and consumer execute seperately.
        We can show the value maybe incorrect by using the following implementation
            \begin{flalign*}
                register_1 &= count\\
                register_1 &= register_1 + 1\\
                count &= register_1\\
            \end{flalign*}
        where $register_1$ is one of the local CPU registers.\\
        Similarly, the statement \texttt{count - 1} is implemented as follows
            \begin{flalign*}
                register_2 &= count\\
                register_2 &= register_2 - 1;\\
                count &= register_2
            \end{flalign*}
        where again, $register_2$ is a local CPU register.\\
        \textcolor{red}{Even though $register_1$ and $register_2$ are local CPU registers, the value of
        this register will be saved and stored by the interrupt handler}\\
        The concurrent execution \texttt{count++} and \texttt{count -- 1} is equivalent
        to the sequential execution where lower evel statements previously are interleaved
        in an arbitrary order, with the order of the high-level statement is preserved.\
        \begin{figure}[H]
            \centering
            \begin{tabular}{c c c c c}
                $T_0$ & $producer$ & execute & $register_1 = count$ & \{$register_1$ = 5\}
            \end{tabular}
        \end{figure}
    \chapter{Safety Critical Systems}
        \textbf{Safety} is the freedom from conditions that cause death, injury, illness, damage
        to or loss of equipment or property, or environmental harm\\
        Software is inherently not safe or unsafe, however can contribute to unsafe conditions in a
        safety critical system. Such software is \textbf{Safety Critical}\\
        IEEE definition: "Software whose use in a system can result in unacceptable risk. Safety-critical
        software incliudes softwre whose operation or failure to operate can lead to a hazardous state.
        Software intended to reover from hazardous states, and software intended to mitigate the severity
        of an incident"
        \section{MISRA C}
            \begin{itemize}
                \item Motor Industry Software Reliability Association
            \end{itemize}
        \section{NASA Power of 10}
            \begin{itemize}
                \item Avoid complex flow constructs (\textcolor{red}{goto, recursion, jumps})
                \item All loops must have fixed bounds (prevents runaway code)
                \item Avoid \textcolor{red}{heap memory allocation} (no malloc, define everything in main)
                \item Restrict functions to a single page (max 50 lines)
                \item Use a minimum of \textcolor{red}{two} runtime assertions per function
                \item Restrict the scope of data to the smallest possible
                \item Check the return value of all non-void functions, or cast to void to indicate the return is useless
                \item Use the preprocessor sparingly. (\textcolor{red}{DO NOT USE} stdio.h, local.h, abort/exit/system from stdlib.h, time handling from time.h)
                \item Limit pointer use to a \textcolor{red}{single dereference} and \textcolor{red}{DO NOT USE FUNCTION POINTERS}
                \item Compile with all warnings active (Wall, Wextra, etc.) all warnings should then be addressed before release
            \end{itemize}
    \chapter{Distributed Systems}
        A distributed system is a collection of processors that do not share memory or
        a clock. Instead, each node has its own local memory. The nodes communicate over 
        various networks, such as high-speed busses.\\
        Applications of distributed systems vary widely, from providing transparent file access
        inside an organization, to large-scale cloud storage services, to business analysis of 
        trends on large datasets, to parallel processing of scientific data. With the most 
        ubiqiutous form of a distributed system being the Internet.
        \section{Basic Concepts}
            A \textcolor{blue}{distributed system} is a collection of loosely coupled nodes
            interconnected by a communication network. From the point of view of a specific node
            in a distributed system, the rest of the nodes and their respective resources are remote,
            whereas its own resources are local.\\
            Processors are variously called \textbf{nodes, computers, machines, hosts}\\
            \begin{itemize}
                \item \textbf{Site} is the location of the processor
                \item Generally a \textbf{server} has a resource a \textbf{client} node at a different
                site wants to use
            \end{itemize}
            \begin{figure}[H]
                \centering
                \includegraphics{figures/client_server.pdf}
                \caption{A client-server distributed system}
            \end{figure}
        \section{Advantages of Distributed Systems}
            \begin{itemize}
                \item \textbf{Resource Sharing}
                    \begin{itemize}
                        \item Sharing and printing files at remote sites
                        \item Processing information in a distributed database
                        \item Using remote specialized hardware devices
                    \end{itemize}
                \item \textbf{Computation speedup}
                    \begin{itemize}
                        \item \textbf{Load sharing}
                        \item \textbf{Job migration}
                    \end{itemize}
                \item \textbf{Reliability}
                    \subitem Detect and recover from site failure, function transfer,
                    reintegrate failed site.
                \item \textbf{Communication}
                    \subitem \textbf{Message passing}\\
                    \indent \indent All higher-level functions of a standalone system can
                    be expanded to encompass a distributed system
            \end{itemize}
            Computers can be downsized, more flexibility, better user interfaces and easier
            maintenance by moving from a large system to a cluster of smaller systems performing
            distributed computing
        \section{Network-Operating Systems}
        \section{Distributed Operating Systems}
    \chapter{CPU Scheduling}
        CPU scheduling is the basis of multiprogrammed operating systems.
        By switching the CPU among processes, the operating system can make the
        computer more productive.
        \section{Basic Concepts}
            A single core system can only run a single process at a time. Others must wait
            till the CPU's core is free and can be rescheduled.\\
            Multiprogramming is the idea of having a process running at all times to maximise
            CPU utilization.\\
            A process is executed until it must wait, typically for the completion of an I/O 
            request. A simple computer system just idles during this period, waiting compute time,
            no work is accomplished. With multiprogramming, we try to use this time productively.
            By keeping several processes in memory, when one process has to wait, the operating takes 
            the CPU away from the process and gives it to another process. On a multicore system this
            is extended to all processing cores on the system.\\
            Such scheduling is fundamental to an operating system's functionality. Almost all 
            computer resources are scheduled before use. The CPU, being one of the primary resources
            of a computer needs special attention during its scheduling
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.2\linewidth]{figures/cpu_io_burst.pdf}
                \caption{Alternating Sequence of CPU and I/O Bursts}
            \end{figure}
            % \subsubsection{CPU-I/O Burst Cycle}
        \section{CPU Scheduler}
            Whenever the CPU idles, the operating system must select a process in the ready queue to be 
            executed.\\
            This is selected by the \textcolor{blue}{CPU Scheduler}. Which selects a process from the processes
            in memory that are ready to execute, and allocates the CPU to it.\\
            The \textcolor{blue}{short-term scheduler} seelcts from processes in the ready queue and allocates 
            the CPU to one of them.\\
            CPU scheduling happens when
            \begin{itemize}
                \item Switching from running to waiting
                \item Switching from running to ready
                \item Switching from waiting to ready
                \item Terminating
            \end{itemize}
            \subsection{Preemptive and Non-Preemptive Scheduling}
                CPU-scheduling decisions may take place under the following four circumstances
                \begin{enumerate}
                    \item Process switching from running state to the waiting state (I/O request, wait invocation for Terminating
                    a child process)
                    \item Process switching from running to ready state (interrupt)
                    \item Process switching from waiting to ready (I/O completion, release of a semaphore)
                    \item Process terminating
                \end{enumerate}
                For circumstances 1 and 4, there is no choice in scheduling, a new process (if exists) must be
                selected for execution. For circumstances 2 and 3, a choice exists.\\
                Circumstance 1, and 4 are called \textcolor{blue}{non preemptive} or \textcolor{blue}{cooperative}.
                Otherwise it is \textcolor{blue}{preemptive}.\\
                \textcolor{blue}{Non-preemptive} scheduling keeps the CPU once the process is allocated till termination
                or switching to the wait state.

                \subsubsection{Preemptive Scheduling}
                    Virtually all modern operating systems use preemptive scheduling.\\
                    However, Preemptive scheduling can result in \textcolor{red}{race conditions} when
                    data is shared between processes (first process is updating data while the second process
                    tries to read it, which is in an inconsistent state).\\
                    Kernel design must be modified to accomodate a preemptive scheduler as race conditions
                    in updating or modifying important kernel data (IO queues, process table, etc.) while they 
                    are being read can cause the system to crash.\\
                    Therefore, a preemptive kernel requires mechanisms such as mutex locks to prevent race conditions
                    when accessing shared kernel data structues.\\
                    Most modern operating systems are \textcolor{blue}{full prememptive} when running in kernel mode.
                \subsubsection{Non Preemptive Scheduling}
                    Non preemptive scheduling is simpler than preemptive scheduling as there is no need 
                    for any special hardware or kernel mode privileges.\\
                    However, non preemptive scheduling is not suitable for modern operating systems as it   
                    does not allow the kernel to respond to interrupts while running in kernel mode.\\
                    Non preemptive scheduling is used in embedded systems where the kernel is the only process
                    running on the system.\\
                    Non preemptive scheduling is also used in real time systems where the kernel is the only process 
                    running on the system.\\
        \section{Dispatcher}
            Is the module that gives control of the CPU's core to the process selected by the scheduler.
            Involves
            \begin{itemize}
                \item Context switching
                \item Switching to user model
                \item Jumping to the proper location in the user program to resume that program
            \end{itemize}
            Speed is crucial with the dispatcher as it is invoked during every context switch.\\
            \textcolor{blue}{Dispatch latency} is the time taken for the dispatcher to stop one process and
            starty another running.\\
        \section{Scheduling}
            Optimisation objectives
            \begin{itemize}
                \item Max CPU utilization
                \item Max throughput
                \item Min turnaround time
                \item Min waiting time
                \item Min response time
            \end{itemize}
            \subsection{First-Come, First-Served (FCFS) Scheduling}
                The process that requests the CPU first is allocated the CPU first.\\
                Managed using a FIFO queue.\\
                Given the following process table
                \begin{figure}[H]
                    \centering
                    \begin{subfigure}{0.5\textwidth}
                        \centering
                        \begin{tabular}{ccc}
                            Process & Burst Time & Arrival Time\\
                            \toprule
                            P1 & 24 & 0\\
                            P2 & 3  & 1\\
                            P3 & 3  & 2\\
                            \bottomrule
                        \end{tabular}
                        \caption{Process Table}
                    \end{subfigure}%
                    \begin{subfigure}{0.5\textwidth}
                        \centering
                        \includegraphics[width=\linewidth]{figures/fifo_gantt_1.pdf}
                        \caption{FIFO Gantt Chart}
                    \end{subfigure}
                    Waiting time for $P_1$ is 0, $P_2$ is 24, $P_3$ is 27. Average waiting time is 17.
                    \caption{Process Table and FIFO Gantt Chart with waiting times}
                \end{figure}
                \begin{figure}[H]
                    \centering
                    \begin{subfigure}{0.5\textwidth}
                        \centering
                        \begin{tabular}{ccc}
                            Process & Burst Time & Arrival Time\\
                            \toprule
                            P1 & 24 & 3\\
                            P2 & 3  & 1\\
                            P3 & 3  & 2\\
                            \bottomrule
                        \end{tabular}
                        \caption{Process Table}
                    \end{subfigure}%
                    \begin{subfigure}{0.5\textwidth}
                        \centering
                        \includegraphics[width=\linewidth]{figures/fifo_gantt_2.pdf}
                        \caption{FIFO Gantt Chart}
                    \end{subfigure}
                    Waiting time for $P_1$ is 6, $P_2$ is 0, $P_3$ is 3. Average waiting time is 3.
                    \caption{Process Table and FIFO Gantt Chart with waiting times}
                \end{figure}
            \subsection{Shortest-Job-First (SJF) Scheduling}
                By associating each process with the length of its next CPU burst, we can schedule the process
                with the shortest time.\\
                SJF is optimal, giving the minimum average waiting time for a given set of processes.\\
                However, knowing the length of the next CPU burst is difficult to predict the length of the next
                CPU burst.\\
                \begin{figure}[H]
                    \centering
                    \begin{subfigure}{0.5\linewidth}
                        \centering
                        \begin{tabular}{cc}
                            Process & Burst Time\\
                            \toprule
                            P1 & 6\\
                            P2 & 8\\
                            P3 & 7\\
                            P4 & 3\\
                        \end{tabular}
                        \caption{Process Table}
                    \end{subfigure}%
                    \begin{subfigure}{0.5\linewidth}
                        \centering
                        \includegraphics[width=\linewidth]{figures/sjf_gantt_1.pdf}
                        \caption{SJF Gantt Chart}
                    \end{subfigure}
                    Average waiting time is 7
                    \caption{Process Table and SJF Gantt Chart with waiting times}
                \end{figure}
                SJF can be either preemptive or non preemptive.\\
                Preemptive SJF is also called \textcolor{blue}{Shortest Remaining Time First}
                \begin{figure}[H]
                    \centering
                    \begin{subfigure}{0.5\linewidth}
                        \begin{tabular}{ccc}
                            Process & Arrival Time & Burst Time\\
                            \toprule
                            P1 & 0 & 8\\
                            P2 & 1 & 4\\
                            P3 & 2 & 9\\
                            P4 & 3 & 5\\
                        \end{tabular}
                        \caption{Process Table with Arrival Time}
                    \end{subfigure}%
                    \begin{subfigure}{0.5\linewidth}
                        \centering
                        \includegraphics[width=\linewidth]{figures/srtf_gantt_1.pdf}
                        \caption{SRTF Gantt Chart}
                    \end{subfigure}
                    Average waiting time is 6.5
                    \caption{Process Table and SRTF Gantt Chart with waiting times}
                \end{figure}
            \subsection{Round Robin Scheduling}
                Each process is given a small unit of CPU time (\textcolor{blue}{time quantum} or \textcolor{blue}{time slice} q) that
                is 10-100 millisecond.
                After that time, the process is preempted and added to the end of the ready queue.\\
                If there are n processes in the ready queue and the time quantum is q, then each process
                gets 1/n of the CPU time in chunks of at most q time units at once. No process waits more
                than (n-1)q time units.\\
                Timer interrupts every quantum to schedule the next process.\\
                \begin{figure}[H]
                    \centering
                    \begin{subfigure}{0.5\linewidth}
                        \centering
                        \begin{tabular}{cc}
                            Process & Burst Time\\
                            \toprule
                            P1 & 24\\
                            P2 & 3\\
                            P3 & 3\\
                        \end{tabular}
                        \caption{Process Table}
                    \end{subfigure}%
                    \begin{subfigure}{0.5\linewidth}
                        \centering
                        \includegraphics{figures/rr_time_quant_4.pdf}
                        \caption{Round Robin Gantt Chart}
                    \end{subfigure}
                    \caption{Round Robin Gantt Chart with time quantum 4}
                \end{figure}
                Typically, round robin has a higher average turnaround than SJF but has better reponse.
    \chapter{Deadlocks}
        \section{System Model}
            A system consists of finite resources distributed among a number of competing threads.
            These resources must be partitioned into several types or classes, each consisting of identical
            instances.\\
            Under normal operation, a thread may utilize a resource in the following sequence
            \begin{enumerate}
                \item \textbf{Request} - The thread requests the resource. If the request cannot be granted 
                immediately (for example, if a mutex lock is held by another thread) the requesting thread must
                wait till it can acquire the resource.
                \item \textbf{Use} - The thread can operate on the resource (for example, if the resource is a mutex
                lock, it can access its critical section)
                \item \textbf{Release} - The thread releases the resource (for example, if the resource is a mutex
                lock, it releases the lock, allowing another thread to acquire it)
            \end{enumerate}
        \section{Deadlock Characterization}
            Deadlocks can arise if four conditions hold simultaneously
            \begin{itemize}
                \item \textbf{Mutual exclusion} - At least one resource must be held in a nonsharable mode;
                that is, only one thread ata time can use the resource. If another thread requests that resource, 
                the requesting thread must be delayed until the resource is released.
                \item \textbf{Hold and Wait} - A thread must be holding at least one resource and waiting to acquire
                additional resources that are currently being hld by other threads
                \item \textbf{No Preemption} - Resources cannot be preempted; resources can only be released voluntarily
                by the thread holding it, after that the thread has completed its task.
                \item \textbf{Circular Wait} - A set of waiting threads must exist such that $T_0$ is waiting for a 
                resource held by $T_1$, $T_1$ is waiting for a resource held by $T_2$, ... $T_{n-1}$ is waiting for a 
                resource held by $T_n$, and $T_n$ is waiting for a resource held by $T_0$.
            \end{itemize}
            Note that circular wait implies hold and wait
        \section{Resource-Allocation Graph}
            A set of vertices V and a set of edges E.
            \begin{itemize}
                \item V is partitioned into two types
                    \subitem P - Set consisting of all the threads in the system
                    \subitem R - Set consisting of all resource types in the system
                \item \textbf{Request edge}
                    \subitem Directed edge $T_i \rightarrow R_j$
                \item \textbf{Assignment edge}
                    \subitem Directed edge $R_j \rightarrow T_i$
            \end{itemize}
            \begin{figure}[H]
                \centering
                \includegraphics{figures/resource_allocation_graph.pdf}
                \caption{Resource Allocation Graph}
            \end{figure}
            \begin{figure}[H]
                \centering
                \begin{subfigure}{0.5\linewidth}
                        \centering
                        \includegraphics{figures/deadlock_resource_alloc_graph.pdf}
                        \caption{Resource Allocation Graph with Deadlock}
                \end{subfigure}%
                \begin{subfigure}{0.5\linewidth}
                        \centering
                        \includegraphics{figures/no_deadlock_resource_alloc_graph.pdf}
                        \caption{Resource Allocation Graph without Deadlock}
                \end{subfigure}
                \caption{Resource Allocation Graphs}
            \end{figure}
            \subsection{Basic Facts}
                \begin{itemize}
                    \item If a graph has no cycles there is no deadlock
                    \item If a graph has a cycle
                        \subitem If only one instance per resource type, then there's a deadlock
                        \subitem If several instances per resource type, then deadlock may or may not exist
                \end{itemize}
        \section{Methods for Handling Deadlocks}
            Ensure the system will never enter a deadlock state, or make sure that once a deadlock occurs,
            it will be recovered from the state. OR ignore the problem and pretend that deadlocks never occur
            in the system (UNIX, Linux, Windows).\\
        \section{Deadlock Prevention}
            Restrain the way requests can be made
            \begin{itemize}
                \item \textbf{Mutual Exclusion} - Not required for sharable resources, must hold for nonsharable
                resources
                \item \textbf{Hold and Wait} - Must guarantee that whenever a thread requests a resource, it does
                not hold any other resources
                    \begin{itemize}
                        \item Require that each thread request all its resources at one time
                        \item Require that once a thread holds a resource, it cannot request additional resources
                        \item Require that once a thread requests a resource, it cannot request additional resources
                        till the requested resource has been allocated to it
                    \end{itemize}
                \item \textbf{No preemption}
                    \begin{itemize}
                        \item If a process that is holding some resources requests another resource that cannot immediately
                        allocated to it, then all resources held must be released.
                        \item Preempted resources are added to the list of resources for which the process is waiting
                        \item Process will be restarted only when it can regain old resources, as as the ones requested
                    \end{itemize}
                \item \textbf{Circular Wait}
                    \begin{itemize}
                        \item Impose a total ordering of all resource types, and require that each thread requests resources
                        in an increasing order of enumeration
                    \end{itemize}
            \end{itemize}
        \section{Deadlock Avoidance}
            Requires that the system has some additional \textbf{a priori} information available.\\
            \begin{itemize}
                \item Simplest and most useful model requires that each process declare the \textbf{maximum number} of resources
                of each type it may need
                \item The deadlock-avoidance algorithm dynamically examines the resource-allocation state to ensure that there can
                never be a circular-wait condition
                \item Resource-allocation state is defined by the number of available and allocated resources, and the maximum demands
                of the processes/threads.
            \end{itemize}
            \subsection{Basic Facts}
                \begin{itemize}
                    \item If a system is in safe state, no deadlocks
                    \item If a system is in unsafe state, possibility of deadlock
                    \item Avoidance, ensure that a system will never enter an unsafe state
                \end{itemize}
        \section{Safe State}
            When a process requests and available resource, the system must decide if immediate allocation leaves the system in 
            a safe state.\\
            The system is in a \textbf{safe state} if there exists a sequence of \textbf{ALL} the processes in the system such that
            for each $P_i$, the resources that $P_i$ can still request can be satisfied by currently available resources + resources 
            held by all the $P_j$, with $j < i$. i.e.,
            \begin{itemize}
                \item If $P_i$ requests a resource, it may have to wait till all the $P_j$ with $j < i$ have finished
                \item When $P_j$ is finished, $P_i$ can obtain its needed resources, execute, return allocated resources, and terminate
                \item When $P_i$ terminates, $P_{i+1}$ can obtain its needed resources, and so on
            \end{itemize}
        \section{Deadlock Avoidance Algorithms}
            \subsection{Resource Allocation Graph Scheme}
                \begin{itemize}
                    \item \textcolor{blue}{Claim edge} $P_i \rightarrow R_j$ indicated that processes $P_i$ may request resource
                    $R_j$.
                    \item Claim edge converts to request edge when a process/thread requests a resource.
                    \item Request edge converted to an assignment edge when the resource is allocated to the process/thread.
                    \item When a resource is released by a process, assignment edge reconverts to a claim edge.
                    \item Resources must be claimed \textit{a priori} in the system.
                \end{itemize}
                \begin{figure}[H]
                    \centering
                    \begin{subfigure}{0.5\linewidth}
                        \centering
                        \includegraphics{figures/resource_allocation_safe.pdf}
                        \caption{Safe State}
                    \end{subfigure}%
                    \begin{subfigure}{0.5\linewidth}
                        \centering
                        \includegraphics{figures/resource_allocation_unsafe.pdf}
                        \caption{Unsafe State}
                    \end{subfigure}
                    \caption{Resource Allocation Graphs}
                \end{figure}
            The request can be granted only if converting the request edge to an assignment edge does not result 
            in the formation of a cycle in the resource allocation graph.
        \section{Banker's Algorithm}
            Used for multiple instances of resources, where each process must a priori claim the maximum number of resources.
            When a process requests a resource it may have to wait. When a process gets all of its resources, it must return them
            in a finite amount of time.
            \subsection{Data Structures}
                With n processes and m resource types, the following data structures are used
                \begin{itemize}
                    \item \textbf{Available} - Vector of length m. If \textit{available[j]} = k,
                    there are k instances of resource type $R_j$ available.
                    \item \textbf{Max} - $n\times m$ matrix. If \textit{Max[i,j]} = k, then process $P_i$ may
                    request at most k instances of resource type $R_j$.
                    \item \textbf{Allocation} - $n\times m$ matrix. If \textit{Allocation[i,j]} = k, then
                    process $P_i$ is currently allocated k instances of $R_j$.
                    \item \textbf{Need} - $n\times m$ matrix. If \textit{Need[i,j]} = k, then process $P_i$
                    may need k more instances of $R_j$ to complete its task.
                \end{itemize}
                \begin{center}
                    \textit{Need[i,j]} = \textit{Max[i,j]} - \textit{Allocation[i,j]}
                \end{center}
            \subsection{Safety Algorithm}
                \begin{enumerate}
                    \item Letting \textbf{Work} and \textbf{Finish} be vectors of length m and n respectively.
                    \item Initialising \textbf{Work} = \textbf{Available} and \textbf{Finish[i]} = false for $i = 0,1,...,n-1$
                    \item Find an \textbf{i} such that \textbf{Finish[i] = false} and \textbf{Need$\leq$Work}
                        \subitem If no such \textbf{i} exists, go to step 4
                    \item \textbf{Work = Work + Allocation[i]}, \textbf{Finish[i] = true}
                        \subitem Go to step 2
                    \item If \textbf{Finish[i] = true} for all \textbf{i}, the system is in a safe state.
                \end{enumerate}
            \subsection{Resource-Request Algorithm for Process $P_i$}
                \textbf{Request[i]} = request vector for process $P_i$. If \textbf{Request[i,j] = k}, then process
                $P_i$ wants k instances of resource type $R_j$.\\
                \begin{enumerate}
                    \item if \textbf{Request[i]} $\leq$ \textbf{Need[i]}, go to step 2. Otherwise, raise error condition
                    since process has exceeded its maxmimum claim.
                    \item if \textbf{Request[i]} $\leq$ \textbf{Available}, go to step 3. Otherwise, $P_i$ must wait as
                    resources are unavailable.
                    \item Pretend to allocate the requested resources to $P_i$ by modifying the state as follows
                        \begin{center}
                            \textbf{Available = Available - Request}\\
                            \textbf{Allocation[i] = Allocation[i] - Request[i]}\\
                            \textbf{Need[i] = Need[i] - Request[i]}\\
                        \end{center}
                        \begin{itemize}
                            \item If safe state, then resources will be allocated to $P_i$
                            \item If unsafe state, $P_i$ must wait, and the old resource-allocation state will be stored
                        \end{itemize}
                \end{enumerate}
            \subsection{Example}
                Given 4 processes [$P_0, P_1, P_2, P_3, P_4$] and 3 resource types [A (10 instances), B (5 instances), C (7 instances)].\\
                At time $t_0$, the following snapshot of the system has been taken\\
                \begin{center}
                    \begin{tabular}{cccc}
                        & Allocation & Max & Available\\
                        \toprule
                        & A B C & A B C & A B C\\
                        $P_0$ & 0 1 0 & 7 5 3 & 3 3 2\\
                        $P_1$ & 2 0 0 & 3 2 2 & \\
                        $P_2$ & 3 0 2 & 9 0 2 & \\
                        $P_3$ & 2 1 1 & 2 2 2 & \\
                        $P_4$ & 0 0 2 & 4 3 3 & \\
                        \bottomrule
                    \end{tabular}
                \end{center}
                The content of matrix \textbf{Need} is calculated as follows
                \begin{center}
                    \begin{tabular}{cccc}
                        & Allocation & Max & Need\\
                        \toprule
                        & A B C & A B C & A B C\\
                        $P_0$ & 0 1 0 & 7 5 3 & 7 4 3\\
                        $P_1$ & 2 0 0 & 3 2 2 & 1 2 2\\
                        $P_2$ & 3 0 2 & 9 0 2 & 6 0 0\\
                        $P_3$ & 2 1 1 & 2 2 2 & 0 1 1\\
                        $P_4$ & 0 0 2 & 4 3 3 & 4 3 1\\
                        \bottomrule
                    \end{tabular}
                \end{center}
                This system is in a safe state since the sequence $<$ $P_1$, $P_3$, $P_4$, $P_0$, $P_2$ $>$ satisfies the safety algorithm.\\
    \chapter{Virtual Machines}
        \section{Benefits and Features}
            \begin{itemize}
                \item Templating - create an OS + application VM, provide it to customers, use it to create multiple instances
                    \subitem Docker containers, AWS AMIs, etc.
                \item Live Migration - Move a running VM from host to another
                    \subitem No interruption of user access
                \item All these features together $\Longrightarrow$ \textcolor{blue}{cloud computing}
                    \subitem Using APIs, programs tell cloud infrastructure (servers, networking, storage) to create new guests, VMs, virtual desktops
            \end{itemize}
        \section{Types of Virtual Machines}
            Many variations as well as hardware details, assiming VMMs take advantage of hardware features, they can simplify implementation and improve performance
            All Virtual Machines have a \textcolor{blue}{lifecycle}
            \begin{itemize}
                \item Created by the VMM
                \item Resources assigned to it (cores, amount of memory, networking details, storage details)
                \item In type 0 hypervisor, resources are usually dedicated
                \item Other types dedicate or share resource (or mix)
                \item When no longer needed, VMs can be deleted, freeing resources
            \end{itemize}
            Steps are simpler and faster than a physical machine install
            \begin{itemize}
                \item Can lead to \textcolor{blue}{VM sprawl} with lots of VMs, history and state are difficult to track
            \end{itemize}
        \section{Types of VMs}
            \subsection{Type 0 Hypervisor}
                Is an older idea under many names by hardware manufacturers
                    \begin{itemize}
                        \item Partitions, Domains
                    \end{itemize}
                Hardware features must be implemented in firemware, the VMM is in firmware. Smaller feature set than other types
    \listoffigures
    \chapter{Practicals}
        \section{Introduction to Operating Systems}
        \section{Operating System Structures}
        \section{Processes}
        \section{Threads}
            \subsection{Thread Creation}
            Exercise: Modify sampleThread.c. Create a third thread and this time sum up the first 20 numbers {1,2,...20}.
            Practice passing a struct to the thread:
            \begin{minted}{c}
                typedef struct num_thdata {
                    int thread_no;
                    int sum_to;
                } thsum;
            \end{minted}
                \inputminted{c}{code/threads/prac/sampleThread.c}
            \subsection{Producer and Consumer Threads}
            \subsection{Searching Hash Table Values}
            \subsection{OpenMP}
        \section{Synchronisation}
        \section{Safety Critical Systems}
        \section{Distributed Systems}
        \section{CPU Scheduling}
        \section{Deadlocks}
\end{document}